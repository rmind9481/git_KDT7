# =====================================================================================

# 자연어 처리(NLP) 작업을 수행하는 순서는 전처리 → 단어 사전 구축 → 단어 토큰화 → 임베딩 → 모델 학습 → 성능 평가의 순서

# ===================================================================================== 
# 1.전처리
# =====================================================================================

# 전처리는 원시 텍스트 데이터를 모델이 처리할 수 있는 형식으로 변환하는 과정
# 전처리 단계는 모델 학습에 매우 중요한 역할을 하며, 텍스트의 품질을 향상시키고 잡음을 제거하는 데 도움

# 주요작업

# 소문자화: 대소문자 구분을 없애기 위해 모든 텍스트를 소문자로 변환
# 구두점 제거: 구두점 제거하여 단어만 남김
# 불용어 (stopwords) 제거 : '이', '그' '저' 등 과 같은 의미가 없는 불용어 제거
# 정규화 : 같은 의미를 가진 단어들을 하나로 통일하는과정  
# => (갔어요, 와 가다 ) 등 을 하나의 단어로 통합
# 어간추출(Lemmatization) 형태소 분석:  단어의 기본형을 찾거나, 각 단어의 형태소를 분석

# 결론 : 전처리는 텍스트 데이터의 일관성을 확보하고, 모델이 보다 정확한 학습을 할수잇도록 돕니다.

# ===================================================================================== 
# 2.단어사전 구축
# =====================================================================================

# 전처리 된 데이터를 바탕으로 모델이 처리할 단어들을 익덱스화 하는작업

# 텍스트 데이터에서 고유 단어들의 집합(사전) 을 만들고, 각 단어에 고유한 인덱스를 부여

# 주요작업

# 텍스트 데이터에서 빈도수가 높은 단어들을 수집하여 단어 사전을 만듬
# 사전의 크기를 설정하여, 너무 낳은 단어가 사용되지 않도록 할수있음
# 특수토큰 (<NUK> , <PAD> 등) 을 추가하여, 모델이 처리할수 없는 단어에 대한 예외처리를 한다.
# 
# 단어사전은 =>  인덱스 매핑을 제공 , 텍스트 데이터를 수차적인 형태로 변환



# ===================================================================================== 
# 3.단어 토큰화
# =====================================================================================

# 단어 토큰화는 텍스트 데이터를 단어 단위나 형태소 단위로 분리하는 작업
# 이 단계는 텍스트를 모델에 입력할수 있는 형태로 만든다.

# 주요작업

# 텍스트를 단어나 문장으로 분리
# 형태소 분석기나 정규식등을 사용하여 문장을 세부 단위로 나눈다.
# 각 단어를 단어 사전에 맞게 인덱스로 변환


# ===================================================================================== 
# 4. 임베딩
# =====================================================================================

# 임베딩은 단어를 고차원  벡터로 변환하여 머신러닝 모델이 이를 이해할수있도록 하는 과정


# 주요작업

# 단어 인덱스를 임베딩 벡터로 매핑하여, 수치적 표현
# 이 벡터는 모델이 단어 간의 관계를 학습하는데 사용
# Word2Vec, Glove, FastTest 와 같은 임베딩 바ㅐㅇ법을 사용할수 있음,  자체 학습을 위한 방법도 있다.

# ===================================================================================== 
# 5. 모델 학습 
# =====================================================================================

# 임베딩 된 데이터를 바탕으로 모델을 학습하는 단계 
# 모델은 주어진 데이터에서 패턴을 학습하고 이를 기반으로 예측

# 주요작업

# 모델은 주어진 입력데이터 (단어 인덱스 또는 벡터) 를 바탕으로 출력값  
# ex) 텍스트 분류 결과, 감정 분석 결과를 예측
# 손실함수를 정의하고 모델의 파라미터를 최적화한다.
# 학습은 에포크(epoch) 단위로 진행되며, 배치 단위로 데이터를 처리하여 모델을 업데이트

# 모델 학습은 단어와 텍스트의 관계를 학습하고 이후 예측할수 있는 성능좋은 모델을 만들어야된다.

# ===================================================================================== 
# 6. 성능평가 
# =====================================================================================

# 학습된 모델의 성능을 평가하는 단계
# 모델이 얼마나 정확하게 예측하는지 확인하고, 성능을 향상시키기 위한 개선점을 찾는다.

# 주요작업

# 모델의 예측결과를 실제 결과와 비교하여 
# 정확도(Accuracy) , 정밀도 (precision), 재현율(recall) , F1-score 등의 평가 지표를 계산

# 혼동행렬 => 을 사용하여, 모델의 예측 결과를 시각화
# 평가 결과에 따라 모델을 튜닝하거나, 더 나은 하이퍼파라미터를 찾아 성능을 개선

# 성능 평가는 모델의성능을 객관적으로 측정하고, 개선할 수 있는 방향을 제시



# ===================================================================================== 
# 임베딩 
# =====================================================================================
# 
# 단어 임베딩은 단어를 수자의 집합으로 표한하는 방법
# 각 단어의 의미를 고차우너 벡터로 변환하여 표현
# 이렇게 벡터로 변환하는 이유는 머신러닝 모델이 숫자만을 처리할수있기 때문
# 
# =====================================================================================

# 벡터의 차원
# 차원은 벡터가 몇개의 숫자로 이루어 졌는지 의미한다.
# 예를 들어, embedding_dim=128이면 각 단어는 128개의 숫자로 표현
# 이 숫자들은 단어의 의미를 고차원 공간에서 나타내기 위한 값
# 차원이 높다는 것은 벡터가 더 많은 정보를 담을수 있다는 뜻

# 임베딩 차원이 2인 경우
# 단어들이 2개의 숫자 벡터로 표현됩니다. 
# 예를 들어, "사과"는 [0.5, 1.2], "배"는 [0.4, 1.1]과 같은 벡터로 표현될 수 있다.
# =====================================================================================
#
# 차원이 클수록 표현 능력이 강력해짐
# 차원이 클수록 각 단어를 던 정밀하게 표현할수있다.
# 차원이 너무 크면 과적합의 위험
# 차원이 너무 작으면 표현력이 부족함 
# 
# =====================================================================================

# =====================================================================================
# Skip-gram 모델 학습 및 데이터 처리 과정
# =====================================================================================
#  모델 실습 : Skip-gram
# =====================================================================================

# Skip-gram 모델을 사용하여 단어 임베딩을 학습하는 과정
# NLP 언어 모델에서 중요한 역활을 하는 단어 임베딩을 학습
# 활용하는 과정

# =====================================================================================

from torch import nn
# PyTorch의 기본 텐서 라이브러리
# 텐서는 다차원 배열로, 수학적 연산을 수행할 수 있는 데이터 구조


class VanillaSkipgram(nn.Module):

	def __init__(self, vocab_size, embedding_dim):

		super().__init__()

		self.embedding = nn.Embedding(
			# nn.Embedding 모듈은 단어 인덱스를 임베딩 벡터로 변환하는 역할


			num_embeddings= vocab_size,
			# vocab_size 단어 사전의 크기 즉, 모델이 처리할수 있는 단어의 총개수
			# vocab_size=5000이면 5000개의 고유 단어가 사전에 존재
			# => 이 값은 데이터셋에 등장하는 고유 단어수와 일치

			# 모델이 처리할수 있는 단어 수를 결정하고, 임베딩 벡터를 해당 단어수만큼 생성

			embedding_dim=embedding_dim
			# embeddibg_dim => 각 단어를 나타내는 임베딩 벡터의 차원

			# 단어의 의미를 고차원 벡터로 표현하여 모델이 단어 간의 관계를 학습할수있도록 한다.
		
		)
		# nn.Linear 는 임베딩 벡터를 받아 예측값을 계싼하는 완전연결층
		# 임베딩 벡터를 단어 사전의 각 단어에 대한 예측을 출력 
		self.liner = nn.Linear(
			in_features=embedding_dim,
			out_features=vocab_size
			# vocab_size로 설정되어 각 단어의 확률 분포를 출력
		)

	# input_ids 단어의 인덱스를 나타내는 텐서
	# ex) 텍스트 '사과' 100번 인덱스면 
	# input_ids = [100]
	def forward(self, input_ids):
		
		embeddings = self.embedding(input_ids)
		# input_ids 를 임베딩 벡터로 변환한 결과 
		# 각 단어는 embedding_dim 차워의 벡터로 변환
		#
		# 단어를 벡터로 변환하여, 모델이 수치적으로 이해
		output = self.liner(embeddings)
		# 각 단어를 예측할수 있도록 하기 위한 출력값 이값은 후속 단계에서 손실 함수와 비교되어 학습
		return output

# =====================================================================================
#
#  영화 리뷰 데이터세트 전처리
#
# =====================================================================================
import pandas as pd
# 시각화 라이브러리

from Korpora import Korpora
# 자연어 처리(NLP) 작업을 쉽게 수행할수 있도록 여러개의 테그슽 데이터셋을 제공
# 한국어에 적합한 다양한 텍스트 데이터셋을 제공

from konlpy.tag import Okt
# 한국어 자연어 처리를 위한 라이브러리, 현태소 분석기를 제공한다.
# Okt 와 같은 형태소 분석기를 사용하여 한국어 텍스트를 단어,형태소 등으로 분리할수있다.

corpus = Korpora.load('nsmc')
# 'nsmc' 데이터셋을 불러온다.
# Korpora.load('nsmc')는 네이버 영화 리뷰 데이터를 로드
# 영화 리뷰 데이터를 사용하여 감정 분석 모델을 학습하기 위해 필요

corpus = pd.DataFrame(corpus.test)
# 데이터프레임으로 변환

tokenizer = Okt()
# tokenIzer  : Okt() 한국어 형태소 분석기 
# 텍스트를 형태소 단위로 나누는 역활
# 영화 리뷰의 텍스트를 단어 단위로 분리하여 모델이 이해할수 있도록 해야하기 때문

tokens =[tokenizer.morphs(review) for  review in corpus.text]
# tokens 는  corpus.text 에 있는 모든 리뷰를 형태소 단위로 분리하여 만든 리스트
# 형태소 분석을 통해 텍스트 데이터를 모델이 처리할수 있는 형태

# 디버깅용
print(tokens[:3])

# =====================================================================================
#
# 단어 사전 구축
#
# =====================================================================================
from collections import Counter
# collections 모듈에서 Counter 클래스를 가져오는 구문
# Counter는 데이터의 빈도수를 세는 데 유용한 클래스

# counter 클래스 역활
# 리스트,튜플 문자열과 같은 반복 가능한 객체에 대해 
# 각 항목이 몇번 나오는지 세어주는 사전 형식의 객체를 생성
# 기본적으로  항목을 키(key) 로 그항목의 빈도(occurrence)를 값(value)으로 저장

def build_vocab(corpus, n_vocab, special_tokens):

	counter = Counter()
	# Counter 객체를 생성하여 단어의 빈도를 셀준비

	# corpus : 단어 사전 을 만들기 위한 텍스트 데이터, 이미 tokens 형태로 준비된 데이터
	# 텍스트 데이터에서 고유한 단어들을 추출하여 사전을 만들기 위해 필요
	# 타입: 리스트의 리스트 (예: [['이', '영화', '재밌다'], ['정말', '좋다']])

	# 단어 사전을 만들기 위한 토큰화된 텍스트 데이터입니다. 각 문장은 단어들의 리스트로 표현된다.
	# tokens 형태로 준비된 데이터가 입력됩니다.

	# 왜 사용되는지: corpus는 모델이 사용할 단어 사전을 만들기 위해 텍스트 데이터를 제공
	# corpus에서 각 문장에 포함된 단어들의 빈도를 계산하여, 고유 단어를 추출

	# n_vocab
	# 타입 : 정수
	# 단어 사전의 크기입니다. 모델이 사용할 단어의 개수를 제한하는 변수로, 
	# 가장 빈도가 높은 상위 n_vocab개의 단어만 사전에 포함
	# 

	# 왜 사용되는지: 데이터셋에 등장하는 모든 단어를 사전에 포함시키면 너무 많은 단어들이 생기기 때문에,
	# 사전의 크기를 제한하여 모델의 효율성을 높이고, 과적합을 방지


	# special_tokens:
	# 타입: 리스트 (예: ['<unk>'])

	# 모델이 처리할 수 없는 단어들을 처리하는 특수 토큰입니다. 
	# 예를 들어, <unk>는 사전에 없는 단어를 나타내는 토큰입니다.

	# 왜 사용되는지: <unk>와 같은 특수 토큰은 모델이 알 수 없는 단어를 처리하는 데 필요. 
	# 예를 들어, 훈련 데이터에 없는 단어가 입력으로 들어올 때 <unk>로 처리할 수 있다.

# =====================================================================================


	for tokens in corpus:
		counter.update(tokens)
		# tokens에 포함된 단어들의 빈도를 세어 counter에 저장
		# update(): 이 메서드는 리스트나 튜플의 항목들을 추가하여, 각 항목의 빈도를 누적하여 업데이트
	vocab = special_tokens
	
	for token, count in counter.most_common(n_vocab):
		# counter.most_common(n_vocab):
		# Counter 객체의 가장 많이 등장한 항목을 빈도수 순으로 반환.
		# n_vocab에 주어진 값만큼 상위 n_vocab개의 단어를 반환
		# 반환값: (단어, 빈도수) 형태의 튜플 리스트
		vocab.append(token)

		# 왜 사용하는지: 이 메서드는 빈도가 높은 단어들만 선택해서 단어 사전을 만들기 위해 사용
	return vocab


vocab = build_vocab(corpus=tokens,  n_vocab=5000, special_tokens=['<unk>'])

# 
token_to_id = {token : idx for idx, token in enumerate(vocab)}
#enumerate(vocab)
# vocab 리스트에서 각 단어와 그 인덱스를 쌍으로 반환
# (index, token) 형태로 각 단어와 그에 해당하는 인덱스
# 왜 사용하는지: 단어를 숫자 인덱스로 변환하기 위해 필요

# 단어 => 인덱스 매핑을 위한 딕셔너리 
# 예를 들어 
# {'사과': 0, '배':1, '맛있다':2}
# 왜 사용하는지: 텍스트 데이터를 모델이 처리할 수 있도록 숫자 인덱스로 변환

id_to_token = {idx: token for idx, token in enumerate(vocab)}
# 인덱스 → 단어 매핑을 위한 딕셔너리 , token_to_id와 반대
# 예를 들어, 
# {0: '사과', 1: '배', 2: '맛있다'}와 같이 인덱스를 단어
# 왜 사용하는지: 모델의 출력값을 다시 단어로 변환하기 위해 필요
#  모델이 예측한 인덱스를 다시 단어로 바꿔주는 역할


print(vocab[:10])
print(len(vocab))


# =====================================================================================
#
#	Skip - gram 의 단어 쌍 추출
#
# =====================================================================================

# kip-gram 모델을 위해 단어 쌍(word pairs)을 추출하는 함수
#  get_word_pairs를 정의
#  Skip-gram 모델은 중심 단어(center word)와 그 주변의 문맥 단어(context word)들을 이용해 단어 임베딩을 학습하는 방식


# tokens는 각 문장에 포함된 단어들을 제공하여, 중심 단어와 문맥 단어를 추출하는 데 사용
# window_size 
# 중심 단어를 기준으로 몇 개의 **문맥 단어(context word)**를 사용할지를 결정하는 윈도우 크기
# 예를 들어 
# window_size=2이면
#  중심 단어의 앞뒤로 2개씩, 총 4개의 문맥 단어를 사용

# 왜 사용되는지: 중심 단어 주위의 문맥 단어를 얼마나 넓은 범위에서 추출할지를 결정
# window_size=2이면 각 중심 단어와 그 주변 2개의 단어를 문맥으로 취급

# =====================================================================================
# **get_word_pairs**는 텍스트 데이터를 기반으로 단어 쌍을 생성하는 함수로, 단어의 관계를 학습할 수 있도록 합니다.
# =====================================================================================
def get_word_pairs(tokens,window_size):
	pairs = []
	# 단어 쌍을 저장할 빈 리스트를 생성합니다. 이후에 중심 단어와 문맥 단어의 쌍을 이 리스트에 추가
	# tokens에 포함된 각 문장을 순차적으로 처리 
	# tokens는 단어로 나뉜 문장의 리스트이므로, 하나의 문장은 단어들이 나열된 리스트
	for sentence in tokens:
		sentence_length = len(sentence)

		# 현재 문장의 길이, 즉 문장의 단어 개수를 구한다
		# 왜 사용되는지: 
		# 윈도우 크기(window_size)를 이용하여 문맥 단어를 추출할 때, 문장의 길이를 벗어나지 않도록 범위를 제한하기 위해 사용

		for idx, center_word in enumerate(sentence):
			
			# enumerate(sentence)는 문장에서 각 단어의 **인덱스(idx)**와 **단어(center_word)**를 반환
			# 문장의 각 단어에 대해 **중심 단어(center_word)**를 찾는다.
			# 
			# 왜 사용되는지 :
			# 각 단어가 중심 단어가 되어, 그 단어를 기준으로 문맥 단어들을 추출
			# 0번 인덱스부터 시작하므로, max(0, ...)을 사용하여 음수 인덱스가 발생하지 않도록 한다.
			window_star = max(0, idx - window_size)

			# 문맥 윈도우의 시작 인덱스
			# idx - window_size는 중심 단어의 왼쪽 범위
			# 왜 사용되는지: 
			# 문장의 처음 부분에서 문맥 단어를 추출할 때, 음수 인덱스가 나오지 않도록 하여 문장의 범위를 벗어나지 않도록 한다.

			window_end = min(sentence_length, idx + window_size+1)
			# 문맥 윈도우의 끝 인덱스
			# idx + window_size + 1은 중심 단어 오른쪽의 범위
			# min(sentence_length, ...)을 사용하여 문장의 끝을 벗어나지 않도록 합니다.
			# 왜 사용되는지: 
			# 문장의 끝에서 문맥 단어를 추출할 때, 문장의 길이를 넘어서는 인덱스를 방지
			center_word = sentence[idx]
			# 현재 중심 단어를 선택합니다. sentence[idx]는 문장에서 idx 위치에 있는 단어
			# 왜 사용되는지:
			# 현재 단어가 중심단어가 되며, 이 중심 단어를 기준으로 문맥 단어
			# 
			context_word = sentence[window_star:idx] + sentence[idx+1:window_end]
			# 현재 중심 단어(center_word)를 기준으로 문맥 단어들을 추출 
			# window_star부터 idx까지는 중심 단어의 왼쪽 문맥이고, 
			# idx+1부터 window_end까지는 오른쪽 문맥
			# 중심 단어는 제외하고 그 주변 단어들만 **문맥 단어(context_word)**로 선택
			
			# 왜 사용되는지: 중심 단어와 그 주변 단어들 간의 관계를 학습하기 위해 문맥 단어들을 추출
			for context_word in context_word:
				
				pairs.append([center_word, context_word])
				# 중심 단어와 문맥 단어의 쌍을 pairs 리스트 추가
				# 왜 사용되는지: Skip-gram 모델에서 중심 단어와 문맥 단어 쌍을 학습 데이터로 사용하기 위해 이 쌍들을 리스트에 저장
	return pairs
	# pairs는 중심 단어와 문맥 단어의 모든 쌍을 담은 리스트
	# 예를 들어, "사과"를 중심 단어로 하고 주변 문맥 단어를 추출했다면, [("사과", "배"), ("사과", "맛있다")]와 같은 쌍들을 반환
	
	# 왜 필요한지: Skip-gram 모델에서는 중심 단어와 문맥 단어의 관계를 학습하여 단어 임베딩을 얻는다.
	# 이 쌍들은 학습에 필요한 데이터를 제공


word_pairs = get_word_pairs(tokens, window_size=2)
# tokens는 텍스트 데이터를 토큰화한 리스트로, 각 문장은 단어 리스트
# window_size=2는 중심 단어를 기준으로 앞뒤로 2개의 단어를 문맥 단어로 사용할 것
# word_pairs는 중심 단어와 문맥 단어의 쌍 리스트입니다. 예를 들어, "사과"를 중심 단어로, "배", "맛있다"가 문맥 단어
# 리스트 (예: [['사과', '배'], ['사과', '맛있다']])

print(word_pairs[:5])
# =====================================================================================
#
#  인덱스 쌍변환
#
# =====================================================================================
# 
# 중심 단어와 문맥 단어로 구성된 단어 쌍 리스트
# 이 값은 get_word_pairs 함수에서 생성된 결과물
# 각 단어 쌍은 skip-gram 모델에서 중심 단어와 그 주변 문맥 단어의 관계
#
# =====================================================================================
# **get_index_pairs**는 단어 쌍을 인덱스 쌍으로 변환하는 함수로, 모델에 입력할 수 있는 숫자 인덱스 형태로 데이터를 변환
# =====================================================================================

# word_pairs 예: [['사과', '배'], ['사과', '맛있다']]
# 중심 단어와 문맥 단어로 구성된 단어 쌍 리스트 get_word_pairs 함수에서 생성된 결과물
# 왜 필요한지: 이 리스트에서 각 단어 쌍을 인덱스로 변환해야 하므로 필요

# token_id
# 딕셔너리 (예: {'사과': 0, '배': 1, '맛있다': 2, '<unk>': 3})
# 단어 → 인덱스 매핑을 위한 딕셔너리입니다. 단어 사전에서 각 단어를 고유한 숫자 인덱스로 변환하는 데 사용
# 텍스트 데이터는 단어로 구성되어 있지만 모델은 숫자 데이터를 처리 할수 있으므로
# 단어를 인덱스로 변환하는데 사용

def get_index_pairs(word_pairs, token_to_id):
	pairs = []
	# 단어 쌍을 인덱스 쌍으로 변환한 결과를 저장할 빈 리스트 생성
	# 왜 필요한지
	# 변환된 인덱스 쌍을 모아서 반환하기 위해 필요
	 
	unk_index = token_to_id['<unk>']
	# '<unk>' 토큰에 해당하는 인덱스를 가져온다.
	# token_to_id <unk> 는 모델이 처리할수 없는 단어를 나타내는 특별한 토큰
	# 왜 필요한가?
	# 사전에 없는 단어를 처리하기 위해 
	# 이를 통해 모델이 알지 못하는 단어를 처리할수 있다.

	for word_pair in word_pairs:
		# word_pairs 에서 각 단어 쌍을 순차적으로 처리
		# 왜 필요한가?
		# 각 단어 쌍을 인덱스로 변환하기 위해 순차적으로 처리
		# 
		center_word, context_word = word_pair
		# word_pair 는 중심단어 와 문맥 단어로 이루어진 튜플
		# 왜 필요한가?
		# 각 단어 쌍을 중심 단어와 문맥 단어로 분리하여, 각 단어를 인덱스로 변환해야 하기 때문에 필요

		

		center_index = token_to_id.get(context_word, unk_index)
		# context_word를 token_to_id에서 찾아 해당하는 인덱스를 가져옴
		# context_word가 token_to_id에 없다면, unk_index를 사용하여 <unk> 인덱스를 대체

		# 왜 필요한가?
		# 모델에 입력할 수 있도록 단어를 인덱스로 변환하기 위해 사용
		
		pairs.append([center_index, center_index])
		# 인덱스 쌍을 모은 pairs 리스트를 반환
		# 왜 필요한가?
		# 인덱스 쌍은 모델 학습에 필요한 필요한 데이터로 이후 모델이 단어 간의 관계를 학습하는데 사용
		

	return pairs

index_pairs = get_index_pairs(word_pairs, token_to_id)
# get_index_pairs 함수는 word_pairs 에서 중심 단어와 문맥 단어를 인덱스로 변환 index_pairs라는 새로운 리스트를 생성
# 왜 필요한지: 
# 모델 학습에서 단어 인덱스를 사용해야 하므로,  단어를 숫자 인덱스로 변환하여 모델에 입력할 수 있도록 한다.

print(index_pairs[:5])



# ==================================================================================================================

# 			 get_word_pairs 									get_index_pairs

# 목적	:	중심 단어와 문맥 단어를 단어쌍으로 추출					  목적: 단어 쌍을 인덱스 쌍으로 변환
# 입력데이터 : 토큰화된 단어 데이터					  			    입력데이터 : get_word_pairs
# 출력데이터 : 단어쌍( 예 [('사과,'배'),('사과','맛있다')])			 출력데이터 : 인덱스 쌍(예 [(0,1),(0,2)])
# 주요기능	 : 단어 쌍을 추출하여 Skip-gram 학습 데이터 준비		  주요기능  : 단어쌍을 인덱스로 변환하여 모델 학습에 필요한 형태로 반환
# 핵심 작업  : 문장에서 중심 단어와 문맥 단어를 추출				  핵심 작업 : 단어를 인덱스로 변환하고, 인덱스 쌍으로 생성 

# ==================================================================================================================



# =====================================================================================
#  데이터로더 적용
# =====================================================================================

import torch
from torch.utils.data import TensorDataset, DataLoader

index_pairs = torch.tensor(index_pairs)
# index_pairs
# 단어 쌍을 인덱스 쌍으로 변환한 결과입니다. 각 쌍은 중심 단어와 문맥 단어의 인덱스입니다.
# 왜 사용되는지: 모델 학습을 위해 단어 인덱스를 사용해야 하므로, index_pairs는 모델의 입력 데이터로 사용됩니다.

center_indexes = index_pairs[:,0]
# index_pairs에서 중심 단어 인덱스(center_indexes)와 
# 문맥 단어 인덱스(context_indexes)를 분리하여 저장

# 왜 사용되는지
# center_indexes와 context_indexes는 모델의 입력과 타겟으로 사용되기 때문에 분리

context_indexes = index_pairs[:,1]

# 왜 사용되는지: center_indexes와 context_indexes는 모델의 입력과 타겟으로 사용되기 때문에 분리

dataset = TensorDataset(center_indexes, context_indexes)
# TensorDataset은 PyTorch 텐서를 데이터셋으로 변환해주는 클래스
# 입력과 타겟 데이터를 하나의 텐서로 묶어서 데이터셋

# 왜 사용되는지
# 텐서 형태의 데이터를 배치 단위로 모델

dataLoader = DataLoader(dataset, batch_size=32, shuffle=True)
# DataLoader는 배치 단위로 데이터를 미니배치 처리하여 효율적인 학습을 도와주는 객체




# =====================================================================================
#  Skip-gram 모델 준비작업
# =====================================================================================

from torch import optim

device = 'cuda' if torch.cuda.is_available() else 'cpu'
# GPU와 CPU를 구분하여, 학습이 가능한 경우 GPU를 사용하고, 그렇지 않으면 CPU를 사용
# 왜 사용되는지
# 모델 학습을 GPU에서 실행할 수 있다면 속도가 매우 빨라지기 때문에 장비에 맞는 최적화를 위해 사용

word2vec = VanillaSkipgram(vocab_size=len(token_to_id), embedding_dim=128).to(device)
# VanillaSkipgram 클래스에서 정의한 Skip-gram 모델을 인스턴스화
# vocab_size는 단어 사전의 크기이고, embedding_dim은 임베딩 벡터의 차원
# 왜 사용되는지
# Skip-gram 모델을 학습시키기 위해 단어 임베딩을 학습할 수 있도록 모델을 생성

criterion = nn.CrossEntropyLoss().to(device)
# 손실 함수로 **CrossEntropyLoss**를 사용
# 분류 문제에서 주로 사용되는 손실함수 
# 예측한 확률 분포와 실제 값 간의 차이
# 왜 사용되는지:
# 모델의 예측값과 실제값 간의 차이를 계산하여 모델의 성능측정

optimizer = optim.SGD(word2vec.parameters(), lr=0.1)
# ptim.SGD는 **확률적 경사하강법(SGD)**을 사용하여 모델의 가중치 파라미터를 업데이트
# 매개변수  word2vec.parameters()
# 파라미터(가중치)를 가져오고, lr=0.1은 학습률을 설정

# 왜 사용되는지
# 모델 파라미터를 최적화 하여 학습을 진행

# =====================================================================================
#  모델 학습
# =====================================================================================

for epoch in range(10):
	# 모델을 10 에포크(10번 반복) 동안 학습
	# 에포크는 학습 데이터 전체가 모델을 한 번 통과하는 과정
	# 왜 사용되는지: 모델을 여러 번 학습시켜야 좋은 성능을 얻을 수 있기 때문에, 에포크를 설정하여 반복 학습
	cost = 0.0

	# DataLoader로부터 
	# **입력 데이터(input_ids)**와 **타겟 데이터(target_ids)**를 배치 단위로 받아온다.
	# 왜 사용되는지: 모델에 **입력값(input_ids)**을 주고, **타겟값(target_ids)**을 통해 모델의 예측을 비교
	for input_ids, target_ids in dataLoader:
		input_ids = input_ids.to(device)
		target_ids = target_ids.to(device)

		logits = word2vec(input_ids)
		# input_ids를 넣어 예측값(로짓)을 계산합니다. 로짓은 각 단어에 대한 예측 확률을 나타낸다.
		# 왜 사용되는지: 모델의 예측값을 계산하여 손실 함수와 비교하기 위해 사용
		loss = criterion(logits, target_ids)
		# logits와 target_ids 간의 차이를 손실 함수인 CrossEntropyLoss로 계산
		# 왜 사용되는지: 모델이 얼마나 잘 예측했는지 오차를 계산하고, 이 값을 최소화하려고 모델을 업데이트합니다.


		optimizer.zero_grad()
		# 기존의 기울기(gradient)를 초기화
		# PyTorch는 기본적으로 기울기를 누적하므로, 새로운 미니배치마다 기울기를 초기화 해야한다.
		# 왜 사용되는지: 이전 배치의 기울기 값이 누적되지 않도록 초기화
		loss.backward()
		# *역전파(backpropagation)**를 통해 **기울기(gradient)**를 계산
		# 모델의 가중치가 어떤 방향으로 조정되어야 하는지 알 수 있다.
		# 왜 사용되는지: 손실을 줄이기 위해 기울기를 계산하여, 모델 가중치를 업데이트
		optimizer.step()
		# 계산된 기울기를 사용하여 모델의 파라미터를 업데이트
		# 왜 사용되는지: 경사하강법을 사용하여 모델의 가중치를 업데이트

		cost += loss
		# 각 배치에서 계산된 손실 값을 누적하여, 에포크마다 평균 손실을 계산
		# 왜 사용되는지: 학습이 진행되면서 손실 값이 줄어들어야 하므로, 학습 과정을 모니터링할 수 있습니다.

	cost = cost/ len(dataLoader)

	print(f'Epoch :{epoch+1:4d}, Cost: {cost:.3f}')

# =====================================================================================
#  임베딩 값 추출
# =====================================================================================

token_to_embeding = dict()
# 단어 → 임베딩 벡터를 매핑하는 딕셔너리
# 왜 사용되는지: 각 단어에 대해 학습된 임베딩 벡터를 매핑하여, 단어와 임베딩을 연결할 수 있도록 한다.

embedding_matrix = word2vec.embedding.weight.detach().cpu().numpy()
# 학습된 임베딩 벡터들을 NumPy 배열로 변환하여 embedding_matrix에 저장
# 왜 사용되는지: 임베딩 벡터는 모델 학습 후 단어 의미를 반영하는 중요한 결과물이기 때문에, 이를 수치적으로 저장


# word2vec 모델의 임베딩 층에 해당하는 파라미터
# 모델 학습 후, 이 임베딩 벡터들이 단어에 대한 의미를 반영
# 왜 사용되는지: 모델 학습을 통해 단어 임베딩 벡터를 추출하고 이를 확인하려고



# 학습된 단어 임베딩 벡터를 각 단어에 매핑하여 단어 → 임베딩 벡터를 연결하는 작업
# 이과정은 학습된 단어 임베딩을 실제로 활용하기 위해 단어와 임베딩을 연결하는 중요한 역할

# vocab은 단어 사전 예를 들어, vocab은 [<unk>, 사과, 배, 맛있다, ...]와 같은 형태로, 모든 단어와 특수 토큰들이 포함된 리스트
# 왜 사용되는지: vocab은 단어에 대한 임베딩 벡터를 얻기 위한 기준이 되는 단어 리스트


# embedding_matrix
# embedding_matrix는 학습이 완료된 단어 임베딩 벡터를 포함하는 배열
# word2vec.embedding.weight.detach().cpu().numpy()로 얻은 이 값은 각 단어의 128차원 임베딩 벡터와 같은 형태로, 단어에 대한 의미를 고차원 공간에 매핑한 값
# 왜 사용되는지
# embedding_matrix는 학습된 임베딩 벡터들이 저장된 **행렬(matrix)**로, 각 단어를 고차원 벡터로 표현한 결과물
for word, embedding in zip(vocab, embedding_matrix):
	# zip(vocab, embedding_matrix)
	# zip 함수는 두 개의 리스트(vocab과 embedding_matrix)를 짝지어 튜플로 묶어 반환
	# 각 단어와 그에 대응하는 임베딩 벡터가 튜플로 결합
	token_to_embeding[word] = embedding
	# token_to_embeding은 단어와 그 단어에 해당하는 임베딩 벡터를 매핑하는 딕셔너리
	# word가 키(key)가 되고, embedding이 값(value)이 되어 딕셔너리에 저장

# 전체적으로 이 코드가 하는 일
# 결과적으로, token_to_embeding 딕셔너리에는 
# 단어 → 임베딩 벡터 매핑이 이루어집니다.
#  예를 들어, '사과'라는 단어는 [0.1, -0.2]라는 임베딩 벡터와 연결
# 이 과정을 통해 학습된 임베딩 벡터를 단어와 연결하여,
# 이후 모델 학습이 끝난 후, 각 단어의 의미를 고차원 벡터로 표현한 결과물을 사용할 수 있게 됨

index = 30
token = vocab[index]
# 단어 인덱스를 이용하여 특정 단어에 해당하는 임베딩 벡터를 추출
token_embeding = token_to_embeding[token]
# token(선택된 단어)에 해당하는 임베딩 벡터를 token_to_embeding 딕셔너리에서 찾아 **token_embeding**에 저장
print(token)
print(token_embeding)

# print(token): 선택된 단어(token)를 출력하여 어떤 단어를 처리했는지 확인
# print(token_embeding): 선택된 단어에 대한 임베딩 벡터를 출력하여 모델이 학습한 단어의 의미를 벡터로 확인