{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ 자연어 - 한국어 형태소 모델 Spacy ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] 모듈 로딩<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy                        ## 행태소 분석기 모듈 로딩\n",
    "from konlpy.corpus import *         ## konlpy 내 말뭉치 로딩\n",
    "import string                       ## 파이썬 문자열 관련 모듈 로딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] 데이터 및 모델 로딩<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 파일명 \n",
    "LAW_FILE = kolaw.fileids()[0]\n",
    "\n",
    "### 한국어 모델 설정\n",
    "KO_MODEL = 'ko_core_news_sm'\n",
    "\n",
    "### 구두점 및 불필요 기호\n",
    "PUNC    = string.punctuation + \"★\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터 로딩 => 줄 단위로 읽어오기 \n",
    "LAW_CORPUS = kolaw.open(LAW_FILE).readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 형태소 모델 로딩\n",
    "MORPH_MODEL = spacy.load(KO_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] 데이터 확인 및 불필요 데이터 처리<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[정리 전] 356개\n",
      "[정리 후] 344개\n"
     ]
    }
   ],
   "source": [
    "print(f'[정리 전] {len(LAW_CORPUS)}개')\n",
    "NUMS = len(LAW_CORPUS)\n",
    "\n",
    "for idx in range(NUMS-1, 0, -1):\n",
    "    LAW_CORPUS[idx] = LAW_CORPUS[idx].strip()\n",
    "    LAW_CORPUS[idx] = LAW_CORPUS[idx].replace('\\n','')\n",
    "\n",
    "    if not len(LAW_CORPUS[idx]): del LAW_CORPUS[idx]\n",
    "\n",
    "print(f'[정리 후] {len(LAW_CORPUS)}개')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['대한민국헌법\\n',\n",
       " '유구한 역사와 전통에 빛나는 우리 대한국민은 3·1운동으로 건립된 대한민국임시정부의 법통과 불의에 항거한 4·19민주이념을 계승하고, 조국의 민주개혁과 평화적 통일의 사명에 입각하여 정의·인도와 동포애로써 민족의 단결을 공고히 하고, 모든 사회적 폐습과 불의를 타파하며, 자율과 조화를 바탕으로 자유민주적 기본질서를 더욱 확고히 하여 정치·경제·사회·문화의 모든 영역에 있어서 각인의 기회를 균등히 하고, 능력을 최고도로 발휘하게 하며, 자유와 권리에 따르는 책임과 의무를 완수하게 하여, 안으로는 국민생활의 균등한 향상을 기하고 밖으로는 항구적인 세계평화와 인류공영에 이바지함으로써 우리들과 우리들의 자손의 안전과 자유와 행복을 영원히 확보할 것을 다짐하면서 1948년 7월 12일에 제정되고 8차에 걸쳐 개정된 헌법을 이제 국회의 의결을 거쳐 국민투표에 의하여 개정한다.',\n",
       " '제1장 총강',\n",
       " '제1조 ① 대한민국은 민주공화국이다.',\n",
       " '②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.',\n",
       " '제2조 ① 대한민국의 국민이 되는 요건은 법률로 정한다.',\n",
       " '②국가는 법률이 정하는 바에 의하여 재외국민을 보호할 의무를 진다.',\n",
       " '제3조 대한민국의 영토는 한반도와 그 부속도서로 한다.',\n",
       " '제4조 대한민국은 통일을 지향하며, 자유민주적 기본질서에 입각한 평화적 통일 정책을 수립하고 이를 추진한다.',\n",
       " '제5조 ① 대한민국은 국제평화의 유지에 노력하고 침략적 전쟁을 부인한다.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAW_CORPUS[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] 형태소 분석 모델 사용<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [3-1] 형태소 분석 언어별 모델 로딩\n",
    "nlpModel = spacy.load(KO_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text     : 일만    \n",
      "lemma_   : 일+만   \n",
      "pos_     : ADV    \n",
      "tag_     : ncpa+jxc\n",
      "is_stop  : False\n",
      "is_punct : False\n",
      "\n",
      "text     : 하고    \n",
      "lemma_   : 하+고   \n",
      "pos_     : CCONJ  \n",
      "tag_     : pvg+ecc\n",
      "is_stop  : False\n",
      "is_punct : False\n",
      "\n",
      "text     : 놀지    \n",
      "lemma_   : 놀+지   \n",
      "pos_     : VERB   \n",
      "tag_     : pvg+ecx\n",
      "is_stop  : False\n",
      "is_punct : False\n",
      "\n",
      "text     : 않으면   \n",
      "lemma_   : 않+으면  \n",
      "pos_     : SCONJ  \n",
      "tag_     : px+ecs\n",
      "is_stop  : False\n",
      "is_punct : False\n",
      "\n",
      "text     : 바보가   \n",
      "lemma_   : 바보+가  \n",
      "pos_     : VERB   \n",
      "tag_     : pvg+ecx\n",
      "is_stop  : False\n",
      "is_punct : False\n",
      "\n",
      "text     : 된다    \n",
      "lemma_   : 되+ㄴ다  \n",
      "pos_     : AUX    \n",
      "tag_     : px+ef \n",
      "is_stop  : False\n",
      "is_punct : False\n",
      "\n",
      "text     : .     \n",
      "lemma_   : .     \n",
      "pos_     : PUNCT  \n",
      "tag_     : sf    \n",
      "is_stop  : False\n",
      "is_punct : True\n",
      "\n",
      "save_tokens : 0개\n"
     ]
    }
   ],
   "source": [
    "## [3-2] 기본 사용법\n",
    "text = \"일만 하고 놀지 않으면 바보가 된다.\" \n",
    "\n",
    "## 분석결과 Doc 타입으로 반환\n",
    "doc = nlpModel(text)\n",
    "\n",
    "## Doc 타입은 token으로 구성 => token 객체.속성명\n",
    "save_tokens=[]\n",
    "for token in doc:\n",
    "    # 표제어, 단어 품사, 자세한 품사, 불용어 여부 \n",
    "    print(f'text     : {token.text:6}') \n",
    "    print(f'lemma_   : {token.lemma_:6}') \n",
    "    print(f'pos_     : {token.pos_:6} ') \n",
    "    print(f'tag_     : {token.tag_:6}') \n",
    "    print(f'is_stop  : {token.is_stop}') \n",
    "    print(f'is_punct : {token.is_punct}\\n') \n",
    "\n",
    "print(f'save_tokens : {len(save_tokens)}개')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE_TOKENS1 : 6개\n",
      "[일만, 하고, 놀지, 않으면, 바보가, 된다]\n"
     ]
    }
   ],
   "source": [
    "## [3-3] 토큰 추출 => 형태소 분리 + 불용어/구두점 제거 토큰만 추가 \n",
    "SAVE_TOKENS1=[]\n",
    "for token in doc:\n",
    "    # 표제어, 단어 품사, 자세한 품사, 불용어 여부 \n",
    "    if not (token.is_stop or token.is_punct) :  \n",
    "        SAVE_TOKENS1.append(token)\n",
    "\n",
    "print(f'SAVE_TOKENS1 : {len(SAVE_TOKENS1)}개\\n{SAVE_TOKENS1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE_TOKENS2 : 6개\n",
      "['일', '하', '놀', '않', '바보', '되']\n"
     ]
    }
   ],
   "source": [
    "## [3-3] 토큰 추출 =>  형태소 분리 + 불용어/구두점 제거 + 표제어 분리된 토큰 추가 \n",
    "SAVE_TOKENS2=[]\n",
    "for token in doc:\n",
    "    # 표제어, 단어 품사, 자세한 품사, 불용어 여부 \n",
    "    if not (token.is_stop or token.is_punct) :  \n",
    "        wlist = token.lemma_.split('+')\n",
    "        #print(wlist, wlist[0],wlist[1])\n",
    "        SAVE_TOKENS2.append(wlist[0])\n",
    "\n",
    "print(f'SAVE_TOKENS2 : {len(SAVE_TOKENS2)}개\\n{SAVE_TOKENS2}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
