# ==================================================================================

# 자연어 전처리 - 정제 + 토큰화
# 정제(불필요한 문자나 기호 제거) 와 토큰화( 문장을 단어로 분리) 


# ==================================================================================
# 	모듈로딩
# ==================================================================================
from nltk.tokenize import sent_tokenize,word_tokenize
# sent_tokenize : 텍스트로를 문장 단위로 분할하는 함수
# word_tokenize : 텍스트로 단어 단위로 분할하는 함수
from nltk.tag import pos_tag
# pos_tag : 단어의 품사를 태깅하는 함수 
from nltk.corpus import stopwords
# stopwords: 불용어를 포함한 목록을 불러오는 기능
from string import punctuation
# punctuation: 문자열에서 구두점을 나타내는 상수
from nltk.stem import WordNetLemmatizer
# WordNetLemmatizer : 명사/형용사/동사 사전기반 원형 복원 클래스

## 데이터 파일
# 말뭉치 즉 코퍼스(Corpus)
DATA_FILE   = '../Data/test_data.txt'           ## 말뭉치 즉 코퍼스(Copous)     
# 영어 불용어 리스트를 가져온다
# stopwords.words(;english) 는  NLTK 의 영어 불용어를 포함하는 리스트를 반환
STOP_WORD   = stopwords.words('english')        ## 불용어 즉, 분석에 의미없는 단어들
# string.punctuation 에서 제공하는 구두점 문자들을 변수에 할당
PUNCTUATION = punctuation                       ## 구두점
# ==================================================================================

print(f'STOP_WORD   : {len(STOP_WORD)}개\n{STOP_WORD}')
# 길이와 불용어의 표시
print(f'PUNCTUATION : {len(PUNCTUATION)}개\n{PUNCTUATION}')
# 길이와 해당 구두점 표시

# 분석할 데이터 파일 경로와 불용어 및 구두점 리스트를 설정
#  불용어와 구두점의 개수 및 내용을 출력하여, 구두점들이 포함되어있는지 디버깅
# ==================================================================================
# [2] 텍스트 데이터 정제
# ==================================================================================
# 대소문자 일치: 텍스트 데이터를 처리하기 전에 모든 문자를 소문자로 변환하여 일관성을 유지
# 기호,문자 제거: 불필요한 문자나 기호를 제거 하여 분석할 텍스트만 남김

## - 파일 데이터 읽어오기
# DATA_FILE 경로에 있는 텍스트파일을 읽기 모드로 utf-8인코딩
with open(DATA_FILE, mode='r', encoding='utf-8') as f:
    # 파일을 읽어서 전체 내용을 변수에 저장
	fileData=f.read()

##- 확인
print(f'fileData==> {len(fileData)}개')


# 소문자 변환
fileData=fileData.lower()
# 디버깅용
#텍스트 데이터의 앞 30자를 출력하여, 대소문자 변환이 잘 되었는지 확인
print(f'fileData==>\n {fileData[:30]}')

# ==================================================================================
# [3] 문장 단위 토큰화 및 불용어 제거 + 원형 복원
# ==================================================================================
sentences = sent_tokenize(fileData)
print(sentences)

# sent_tokenize 는 주어진 텍스트를 문장 단위로 분리하여 리스트로 반환
# sentences : 분리된 문장들을 sentences 변수의 내용

# ==================================================================================
# 함수기능 : 품사 기반 불필요 품사 제거 + 원형 복원 토큰반환
# 함수 이름 : covertOriginal
# 매개 변수 : pos_token_list - (단어, 품사) 형태의 토큰 리스트
# 함수 결과 : 불필요 품사 제거 + 원형 복원 토큰 리스트
# ==================================================================================

# 이함수는 품사 기반으로 불필요한 품사를 제거하고 원형 복원
# pos_token_list 는 (단어,품사) 형식의 리스트로 들어옴

def covertOriginal(pos_token_list):

	# 표제어 추출 인스턴스
	wnLemma = WordNetLemmatizer()
	
	# 원형복원 저장
	result = []
	print(pos_token_list)
	# 형용사, 동사 경우 표제어 즉, 원형 복원
	for word, pos in pos_token_list:

		# 품사 태킹을 이용하여 형용사(JJ) 동사(VB)일 경우
		# WordNetLemmatizer 를 사용해 해당 단어를 원형으로 변환

		if pos[:2] in ['JJ','VB']:
			result.append(wnLemma.lemmatize(word,'a' if pos=='JJ' else 'v'))
		# 불필요한 품사인 DT(관사), IN(전치사), CD(숫자), CC(접속사) 는 결과에서 제거
		elif pos not in ['DT','IN','CD','CC']:
			# 토큰 합치기 => 불필요한 품사 제거한 토큰들 추가
			result.append(word)
	return result
# ==================================================================================
# 문장 단위 전처리 된 문장 리스트 추출

# 문장별 토큰 리스트, 품사 토큰 리스트 저장
sent_token_list, pos_token_list = [],[]
# sen_token_list : 각 문장에서 처리된 단어들을 저장하는 리스트
# 단어 => 원형 복원 및 불용어 제거 후 남은 단어들

# pos_token_list : 각 문장에서 품사 태깅 결과를 저장할 리스트
# 리스트에는 단어와 그에 해당하는 품사 정보가 포함

for sent in sentences:
	# 문장 단위로 두구점 제거, 토큰 분리
	for p in PUNCTUATION:
		if p in sent: sent=sent.replace(p,'')

	# 토큰 분리 => list 반환
	# 단어 토큰화 및 품사 태깅
	# word_tokenize(sent) 문장을 단어 단위로 분리하는 함수
	# pos_tag : 각 단어에 대해 품사를 태깅  => (단어,품사) 
	pos_token_list = pos_tag(word_tokenize(sent))
	# 디버깅용
	print(pos_token_list)

	# 원형 복원 및 불필요한 품사제거
	## 형용사 'JJ'=> 'a', 동사 'VB' => 'v'원형 복원 
    ## 불필요한 품사 제거한 토큰들 저장
	# 원형으로 복원하고 불필요한 품사를 제거
	tokens = covertOriginal(pos_token_list)
	# 디버깅용
	print(tokens)
	
	# 토큰 리스트에서 불용어 제거
	# STOP_WORD 에 포함된 단어들을 제거하여 텍스트에 의미 없는 단어는 제외
	words = [ w for w in tokens if w not in STOP_WORD]
	
	# 토큰화 된 단어들 저장
	# 전처리 후에 남은 단어들을 sent_token_list 에 저장
	sent_token_list.append(words)

print(F'총 문장 수 : {len(sent_token_list)}개\n{sent_token_list}')
# ==================================================================================

#  NLP 에서 전처리 중요한 이유
# 1. 구두점 제거 
# 구두점은 텍스트 분석에서 의미를 갖지 않으므로 제가한다.
# 문장에서 ',' 나 '.' 은 분석에 필요 없고 모델 학습에 방해가 된다.

# 2. 단어 토큰화
# 텍스트를 단어 단위로 분할하는 작업은 NLP의 핵심이다.
# 모델은 단어 단위로 데이터를 처리하므로 문장을 단어로 나누는 작업이 중요

# 3. 품사태깅
# 각 단어의 역할을 이해하기 위해 품사 태깅이 필요하다.
# 예를 들어 'RUN'이라는 단어가 동사로 사용될때와 명사로 사용될때 의미가 다를수 있다.

# 4. 원형 복원
# 텍스트 분석에서 형용사나 동사는 원형으로 변환해야만 더 정확한 분석을 할수있다.
# 'running' 과 'ran' 은 모두 'run' 으로 처리하여 모델이 두 단어를 같은 의미로 인식할수있도록

# 5. 불용어 제거
# 불용어는 분석에서 의미 없는 단어들이다.
# 이러한 단어를 제거하면 텍스트의 의미 있는 단어들만 남겨두어 모델이 더 정확하게 학습

# ==================================================================================

# 단어 사전 생성 

# ==================================================================================

token_list = [ word for sent in sent_token_list for word in sent]
# 목적 : 문장 단위로 분리된 단어들을 하나의 리스트로 합치는 작업

# sent_token_list 는 각 문장의 단어들이 이미 전처리된 상태로 포함된 리스트
# 각 문장은 단어들의 리스트로 되어있다.
print(f'token_list => {token_list}')
print(f'token_list => {len(token_list)}개')

## - 토큰들 중복 제거
# set => 집합으로 변환하여 중복된 값을 제거
token_list = list(set(token_list))
print(f'token_list => {token_list}')
print(f'token_list => {len(token_list)}개')

# 문장단위에서 단어추출
# 문장 단위로 단어들을 분리하고 나면, 전체 단어들을 하나로 합치늰 과정은 모델이 데이터를 처리하기 위한
# 단어 단위 분석을 할수있도록 돕습니다. 이과정은 단어들을 개별적으로 학습

### dict 타입으로 단어사전 생성
# 단어 사전을 딕셔너리  형태로 생성
# 숫자 인덱스를 = > 단어로 변환하는 사전 
VOCAB_TO_IDX = {'<PAD>':0, '<UNK>':1 }
VOCAB_TO_IDX
# VOCAB_TO_IDX 는 단어를 고유한 숫자 인덱스로 매핑하는 사전
# <PAD> , <UNK> 는 특수 토큰을 나타냄

# <PAD> 패딩 토큰으로 문장이 짧아서 길이를 맞추기 위해 추가되는 특수토큰
# 이토큰은 모델 학습에 영향을 미치지않으며, 모델은 이값을 무시한다.
## 딥러닝 시 텐서의 shape를 맞추어야 하기 때문에 반드시 필요함


# <UNK> 알수 없는 단어, 로 훈련데이터에 등장하지 않은 단어가 있을 경우 이 토큰으로 대처
#

# ==================================================================================
# 단어 사전
# ==================================================================================
# 모델의 입력을 위한 숫자화
# 머신러닝 모델은 숫자만 처리 할수 있습니다. 자연어는 본래 문자로 되어있기 때문에
# 모델이 이해할수 있는 형태인 숫자로 변환해야한다.
# 각 단어들 고유한 숫자로 매핑하여, 모델이 이를 처리할수있게 합니다.
# 모델은 각 단어를 숫자로 변환된 벡터를 사용하여 학습을 진행
# 이때 숫자 인덱스를 통해 각 단어의 의미를 잠재적으로 학습


## 토큰들에게 정수 숫자 부여
for idx, token in enumerate(token_list, 2):
    VOCAB_TO_IDX[token]=idx 

print(VOCAB_TO_IDX)

# token_list 에 포함된 모든 단어를 VOCAB_TO_IDX 에 숫자 인덱스와 함께추가하는 작업

# enumerate(token_list,2):
# enumerate() 함수는 token_list 에서 각 단어를 순차적으로 처리합니다.
# enumerate(token_list, 2)는 인덱스를 2부터 시작하도록 설정하여,
# 이미 '<PAD>' (0)과 '<UNK>' (1) 토큰이 VOCAB_TO_IDX에 포함되어 있기 때문에,
# 그다음부터는 2부터 시작해서 각 단어에 고유한 인덱스를 부여함




# 숫자인덱스 => 단어 
## idx => word로 변환
# VOCAB_TO_IDX 에 숫자 인덱스를 단어로 변환하는 반대의 사전을 생성
IDX_TO_TOKEN = {v:k for k,v in VOCAB_TO_IDX.items()}
# VOCAB_TO_IDX.items()는 (단어, 숫자 인덱스) 형태의 튜플을 반환
# v : k for k, v in VOCAB_TO_IDX.items() 는 이 튜플을 반대로 뒤집어
# (숫자 인덱스, 단어) 형태로 변경하여 새로운 사전 IDX_TO_TOKEN 을 만든다.


print('VOCAB_TO_IDX=>\n', VOCAB_TO_IDX)

print('IDX_TO_TOKEN=>\n', IDX_TO_TOKEN)



# ==================================================================================
# 모델은 단어의 숫자 인덱스만을 받기 때문에, 텍스트 데이터를 수치로 변환하는 단어 사전은 NLP
# 모델의 핵심입니다. 이과정 없이는 모델이 텍스트를 이해 할수없다.
# 단어 사전을 사용하여 텍스트 데이터를 일관성 있게 숫자로 변환하고
# 이를 모델에 전달하여 학습
# ==================================================================================

#  자연어 => 숫자 변환 

# ==================================================================================

## 문장별 토큰 ==> 수치화 : 수치벡터화

SENT_NUM_LIST =[]

# sent_token_list : 이 리스트는 전처리된 문장들로 각문장이 단어들의 리스트로 되어 있다.
for sent_line in sent_token_list:
# VOCAB_TO_IDX : 이 사전은 각 단어를 수자 인덱스로 매핑한것
    setnums= [ VOCAB_TO_IDX[word] for word in sent_line]
	# VOCAB_TO_IDX 사전을 사용하여 숫자 인덱스로 변환
	# 이 과정에서 각 단어는 해당하는 수자 인게스로 치환

	# ex) sent_line = [ 'hello', 'world'] 이고
	# VOCAB_TO_IDX = {'hello':2, 'world': 3 }
	# 그러면 setnnums = [2,3]


	# 현재 처리 중인 문장을 출력
    print('\n[문장]', sent_line)
	# 수치화된 문장을 출력, 단어가 숫자 인덱스로 변환된 결과
    print('[수치]', setnums)
    SENT_NUM_LIST.append(setnums)

# 왜 이 작업이 중요한가?
# 단어를 숫자로 변환하는 과정은 모델에 입력될수 있도록 데이터를 준비하는 중요한 작업
# 대분분의 머신러닝 및 딥러닝 모델은 숫자 데이터를 입력으로 받는다.
# 따라서 단어를 숫자 인덱스로 변환하여 모델에 적합한 형태로 만들어야된다.

# 단어 사전에서 이미 숫자로 변환했는데
# 자연어를 => 숫자로 변화하는 이유

# 차이점: 단어 사전 만들기와 문장별 토큰을 수치화하기

# 1. 단어 사전만들기
# 단어 사전 만들기는 전체 텍스트에서 고유한 단어들을 추출하고
# 그 단어들에 고유한 숫자 인덱스를 할당하는 작업


# 2. 문장별 토큰을 수치화
# 문장별 토큰을 수치화 하는 과정에서는 이미 만들어진 단어사전을 이용해
# 각 문장을 숫자 형태로 변환하는 작업
# 한 번만 생성되며, 이후 모든 문장에서 동일한 단어가 등장할 때마다 같은 숫자 인덱스로 변환됩니다.

# 단어 사전 만들기는 단어와 숫자 인덱스 간의 매핑을 정의하는 작업입니다.

# 문장별 토큰을 수치화하는 작업은 각 문장을 숫자 인덱스로 변환하여, 모델이 텍스트를 이해할 수 있도록 준비하는 작업입니다.


## 숫자 문장들 체크
for _ in SENT_NUM_LIST: print( _ )


# ==================================================================================
# 패딩 : 모든 문장 길이 맞추기
# ==================================================================================
# 목적 : 이코드는 각 문장의 단어 개수를 계산하고
# 가장 긴문장과 가장 짧은 문장을 확인하는 작업

# 패딩의 필요성:
# NLP 모델에서는 모든 입력 문장의 길이가 동일해야 합니다. 
# 왜냐하면 **배치 처리(batch processing)**를 할 때, 
# 모델이 모든 문장을 동일한 길이로 처리해야 하기 때문입니다.
# 만약 각 문장이 다른 길이를 가진다면, 불일치로 인해 모델이 학습할 수 없습니다.


# 예를 들어, 가장 긴 문장이 5개의 단어를 포함하고, 
# 가장 짧은 문장이 2개의 단어를 포함한다고 가정해봅시다.
# 길이를 맞추기 위한 패딩은 짧은 문장에 추가하여,
# 모든 문장이 동일한 길이 5를 갖도록 만듭니다.


## 문자별 단어 갯수 체크
length  = [ len(sent) for sent in SENT_NUM_LIST]

print(f'문장별 단어 개수 :  {length}')
print(f'가장 긴 문장    :  {max(length)}, 가장 짧은 문장 :{min(length)}')

## 문장별 길이 일치 => 가장 긴문장

# 문장들중에서 가장 긴문장의 길이 (단어수) 를 MAX_LEN 에저장
MAX_LEN = max(length)

NUMS = len(SENT_NUM_LIST)
# SENT_NUM_LIST에 저장된 문장 개수를 NUMS에 저장

# 각 문장을 순차적으로 처리
for idx in range(NUMS):
	# 현재 문장의 길이를 sent_len에 저장
    sent_len = len(SENT_NUM_LIST[idx])
	# 문장의 길이가 가장 긴 문장의 길이(MAX_LEN)과 다르면
	# 패딩을 추가
    if sent_len != MAX_LEN:
		# 문장이 가장 긴 문장보다 짧으면, MAX_LEN - sent_len만큼 패딩을 추가
        for _ in range(MAX_LEN-sent_len):
			# VOCAB_TO_IDX['<PAD>']는 패딩 토큰을 나타내는 숫자 인덱스
            SENT_NUM_LIST[idx].append( VOCAB_TO_IDX['<PAD>'])
			# 문장에 패딩을 추가합니다. 
			# 예를 들어, SENT_NUM_LIST[idx]가 [2, 3]이라면, 패딩 후 [2, 3, 0, 0, 0]으로 변환
			
			# 예시 1: 문장 길이 맞추기 전
			# SENT_NUM_LIST = [[2, 3], [4, 5, 6, 7, 8]]
			# length = [2, 5] => 가장 긴 문장 길이는 5
			
			# 예시 2: 패딩 추가후
			# [2,3,0,0,0] (패딩추가)
			# [4, 5, 6, 7, 8] 는 길이가 이미 5 이므로 그래로 유지


# 디버깅용
for _ in SENT_NUM_LIST:
    print(f'{len(_)}개 : {_}')



# 패딩이 중요한 이유:
# 입력 길이 일정: NLP 모델은 입력 길이가 일정해야 학습할 수 있습니다.
# 문장의 길이가 일정하지 않으면, 모델은 배치 처리가 불가능하거나 문장별로 길이를 맞추는 과정에서 문제가 발생합니다.

# 배치 처리: 배치 처리는 여러 문장을 한 번에 처리하는 방법으로, 학습 속도를 크게 향상시킵니다. 
# 배치 처리를 위해서는 모든 문장의 길이가 같아야 합니다. 패딩은 이를 가능하게 만듭니다.

# 효율성: 패딩을 통해 짧은 문장의 길이를 자동으로 맞추기 때문에, 문장의 길이가 일정하지 않아도 모델이 일관성 있게 처리할 수 있습니다


