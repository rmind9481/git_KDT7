{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ Pytorch DL MODEL 실습 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터셋 : iris.csv \n",
    "- 학습방법 : 지도학습 + 분류 ==>  다중분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] 모듈로딩 및 데이터 준비<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모듈로딩\n",
    "import pandas as pd                                     ## 데이터 관련 모듈들\n",
    "import numpy as np \n",
    "\n",
    "import torch                                            ## Tensor 및 기본 함수들 관련 모듈들\n",
    "import torch.nn as nn                                   ## 인공신경망 관련 모듈들\n",
    "import torch.nn.functional as F                         ## 인공신경망 관련 함수들\n",
    "import torch.optim as optim                             ## 최적화 모듈\n",
    "from torch.utils.data import Dataset, DataLoader        ## 학습 데이터셋 관련 모듈\n",
    "\n",
    "from sklearn.model_selection import train_test_split    ## 학습용 데이터셋 관련 함수\n",
    "\n",
    "from torchmetrics.classification import *               ## 성능지표 모듈 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터 준비\n",
    "DATA_FILE = '../Data/iris.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] 데이터 로딩 및 확인<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal.length  150 non-null    float64\n",
      " 1   sepal.width   150 non-null    float64\n",
      " 2   petal.length  150 non-null    float64\n",
      " 3   petal.width   150 non-null    float64\n",
      " 4   variety       150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n"
     ]
    }
   ],
   "source": [
    "## 데이터 로딩\n",
    "irisDF = pd.read_csv(DATA_FILE)\n",
    "\n",
    "irisDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Setosa', 'Versicolor', 'Virginica'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 데이터 확인\n",
    "irisDF.variety.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "irisDF.variety=irisDF.variety.replace({'Setosa':0, 'Versicolor':1, 'Virginica':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal.length    float64\n",
       "sepal.width     float64\n",
       "petal.length    float64\n",
       "petal.width     float64\n",
       "variety           int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irisDF.variety=irisDF.variety.astype('int')\n",
    "irisDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] 데이터 셋 준비<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FeatureDF] (150, 4), 2D\n",
      "[targetSR]  (150,),  1D\n"
     ]
    }
   ],
   "source": [
    "## 피쳐와 타겟 분리 \n",
    "featureDF =irisDF[irisDF.columns[:-1]]   # Feature 4개\n",
    "targetSR = irisDF[irisDF.columns[-1]]    # 품종 1개\n",
    "\n",
    "print(f'[FeatureDF] {featureDF.shape}, {featureDF.ndim}D')\n",
    "print(f'[targetSR]  {targetSR.shape},  {targetSR.ndim}D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X_train] (120, 4), 2D  [y_train] (120,), 1D\n",
      "[X_test]  (30, 4), 2D  [y_test] (30,), 1D\n"
     ]
    }
   ],
   "source": [
    "## 학습용, 테스트용 데이터셋 분리 \n",
    "X_train, X_test, y_train, y_test = train_test_split(featureDF, \n",
    "                                                    targetSR, \n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=targetSR,\n",
    "                                                    random_state=42)\n",
    "\n",
    "print(f'[X_train] {X_train.shape}, {X_train.ndim}D  [y_train] {y_train.shape}, {y_train.ndim}D')\n",
    "print(f'[X_test]  {X_test.shape}, {X_test.ndim}D  [y_test] {y_test.shape}, {y_test.ndim}D')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 인덱스 재설정 \n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Series => DataFrame : to_frame()\n",
    "type(y_train.to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Series[인덱스] => reshape(-1, 1) : 2D ndarray\n",
    "# - 1개 원소 추출 시 원소의 타입 결과 \n",
    "y_train[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - 여러 개 원소 추출 시 Series로 결과 \n",
    "type(y_train[[8,106]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] 커스텀 데이터셋 클래스 설계 및 구현 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## iris 전용 데이터셋 클래스 \n",
    "class IrisDataset(Dataset):\n",
    "    # 피쳐와 타겟 분리 및 전처리 진행 \n",
    "    def __init__(self, featureDF, targetSR):\n",
    "        super().__init__()\n",
    "        self.feature = featureDF\n",
    "        self.target  = targetSR\n",
    "        self.rows = featureDF.shape[0]\n",
    "        self.cols = featureDF.shape[1]\n",
    "    \n",
    "    # 데이터셋의 샘플 수 반환 메서드 \n",
    "    def __len__(self):\n",
    "        return self.rows \n",
    "\n",
    "    # DataLoader에서 batch_size만큼 호출하는 메서드\n",
    "    # 인덱스에 해당하는 피쳐와 타겟 반환 단, Tensor 형태\n",
    "    def __getitem__(self, index):\n",
    "       arrFeature = self.feature.iloc[index].values   # ndarray\n",
    "       arrTarget = self.target[index].reshape(-1)     # ndarray\n",
    "   \n",
    "       return torch.FloatTensor(arrFeature), torch.Tensor(arrTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([4.9000, 2.5000, 4.5000, 1.7000]), tensor([2.]))\n",
      "(tensor([6.1000, 3.0000, 4.9000, 1.8000]), tensor([2.]))\n"
     ]
    }
   ],
   "source": [
    "## 확인 => Train용 DataSet 필수 \n",
    "##     => Test용 Dataset 선택 => 데이터가 많다면 DS, DL 생성 사용\n",
    "##                           => 데이터가 많지 않다면 DS, DL 생성 필요 X \n",
    "trainDS = IrisDataset(X_train, y_train)\n",
    "testDS  = IrisDataset(X_test, y_test)\n",
    "print( trainDS[1] )\n",
    "print( testDS[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.4000, 2.9000, 1.4000, 0.2000],\n",
      "        [4.9000, 2.5000, 4.5000, 1.7000],\n",
      "        [6.8000, 2.8000, 4.8000, 1.4000]])\n",
      "tensor([[0.],\n",
      "        [2.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "## DataLoader로 확인\n",
    "trainDL = DataLoader(dataset=trainDS, batch_size=3)\n",
    "for feature, label in trainDL:\n",
    "    print(feature, label, sep='\\n')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.4000, 3.0000, 1.3000, 0.2000],\n",
      "        [6.1000, 3.0000, 4.9000, 1.8000],\n",
      "        [4.9000, 2.4000, 3.3000, 1.0000]])\n",
      "tensor([[0.],\n",
      "        [2.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "# 테스트용 데이터로더\n",
    "testDL = DataLoader(dataset=testDS, batch_size=3)\n",
    "for feature, label in testDL:\n",
    "    print(feature, label, sep='\\n')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] 모델 설계 및 클래스 구현<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 클래스 설계 ----------------------------------------------------------------------\n",
    "## 회귀용 커스텀 모델 \n",
    "## 클래스이름 : IrisModel\n",
    "## 부모클래스 : nn.Module\n",
    "## 모델층구성   입력신호/피쳐수       출력신호수/퍼셉트론수       활성화함수\n",
    "## - 입력층  :      4                     100                 ReLu\n",
    "## - 은닉층  :     100                     30                 ReLu \n",
    "## - 출력층  :      30                     3                  Softmax => 손실함수 내 포함\n",
    "## - -----------------------------------------------------------------------------------\n",
    "class IrisModel(nn.Module):\n",
    "    ## 모델 층 설계 및 초기화 메서드 \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.in_layer  = nn.Linear(4,  100)\n",
    "        self.hd_layer  = nn.Linear(100, 30)\n",
    "        self.out_layer = nn.Linear(30,  3)\n",
    "\n",
    "    ## 학습 진행 메서드 \n",
    "    def forward(self, data):\n",
    "        ## 입력층\n",
    "        out = F.relu( self.in_layer(data) )\n",
    "        ## 은닉층\n",
    "        out = F.relu( self.hd_layer(out) )\n",
    "        ## 출력층\n",
    "        return self.out_layer(out) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IrisModel(\n",
      "  (in_layer): Linear(in_features=4, out_features=100, bias=True)\n",
      "  (hd_layer): Linear(in_features=100, out_features=30, bias=True)\n",
      "  (out_layer): Linear(in_features=30, out_features=3, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1476, -0.0370, -0.0230]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 모델 구조 확인\n",
    "model = IrisModel()\n",
    "print( model )\n",
    "\n",
    "## shape 체크\n",
    "data = torch.FloatTensor( [[1.,3.2, 1.9, 2.6]] )   # (1, 4)\n",
    "model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2870,  0.0716, -0.1388], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## shape 체크\n",
    "for feature, target  in testDS:\n",
    "    pre_ = model(feature)\n",
    "    print(pre_)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5] 학습 준비 <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [5-1]학습 관련 설정들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE : cpu, EPOCHS : 70, BATCH_SIZE : 10 ITERATION : 12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EPOCHS      = 70                                    # 학습용 DS을 처음부터 끝까지 1번 학습하는 것을 에포크\n",
    "BATCH_SIZE  = 10                                    # DS을 학습량 만큼 나눈 사이즈 \n",
    "ITERATION   = int(X_train.shape[0]/BATCH_SIZE)      # 학습용 DS이 분리된 수 => 1에포크에 W, b 업데이트 횟수\n",
    "\n",
    "# 학습, 데이터셋 로딩 등 GPU에서 실행 여부 설정 \n",
    "DEVICE      = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f'DEVICE : {DEVICE}, EPOCHS : {EPOCHS}, BATCH_SIZE : {BATCH_SIZE} ITERATION : {ITERATION}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [5-2] 학습 관련 인스턴스들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINDL   = DataLoader(trainDS, batch_size=BATCH_SIZE) ## 학습용 데이터로더\n",
    "TESTDL    = DataLoader(testDS,  batch_size=BATCH_SIZE) ## 테스트용 데이터로더\n",
    "LR        = 0.01  \n",
    "MODEL     = IrisModel()                                ## 학습 모델\n",
    "OPTIMIZER = optim.Adam(MODEL.parameters(), lr=LR)      ## 최적화 즉, 경사하강법 알고리즘으로 W, b의 값 갱신\n",
    "LOSS_FN   = nn.CrossEntropyLoss()                      ## 다중분류 손실함수\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [5-3] 학습관련 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##- 검증 함수 --------------------------------------------\n",
    "##- 검증용 데이터셋으로 모델 검증\n",
    "##- 학습 지속여부 결정 기준이 됨\n",
    "##- -----------------------------------------------------\n",
    "def evaluate():\n",
    "    # 에포크 단위로 검증 => 검증 모드\n",
    "    MODEL.eval()\n",
    "    \n",
    "    # W, b가 업데이트 해제\n",
    "    with torch.no_grad():\n",
    "        # 검증용 데이터셋 => 텐서화 ndarray ==> tensor변환\n",
    "        x = torch.FloatTensor(X_test.values) \n",
    "        y = torch.Tensor(y_test.values)\n",
    "        \n",
    "        # 검증진행\n",
    "        pre_y= MODEL(x)\n",
    "        \n",
    "        # 손실 계산\n",
    "        loss = LOSS_FN(pre_y, y.reshape(-1).long())\n",
    "\n",
    "        # 정확도 계산\n",
    "        accuarcy = MulticlassAccuracy(num_classes=3)\n",
    "        acc = accuarcy(pre_y, y.reshape(-1))\n",
    "\n",
    "        return loss.item(), acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##- 학습 함수 --------------------------------------------\n",
    "##- 학습용 데이터셋으로 모델 검증\n",
    "##- -----------------------------------------------------\n",
    "def training():\n",
    "    # 학습 모드 설정\n",
    "    model.train()\n",
    "\n",
    "    E_LOSS, E_ACC = 0, 0\n",
    "    for feature, target in TRAINDL:\n",
    "        # 배치크기만큼 feature, target로딩\n",
    "        #print('로딩 데이터 :', feature.shape, target.shape)\n",
    "\n",
    "        # 가중치 기울기 0 초기화\n",
    "        OPTIMIZER.zero_grad()\n",
    "\n",
    "        # 학습 진행\n",
    "        pre_y = MODEL(feature)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = LOSS_FN(pre_y, target.reshape(-1).long())\n",
    "\n",
    "        # 정확도 계산\n",
    "        accuarcy = MulticlassAccuracy(num_classes=3)\n",
    "        acc = accuarcy(pre_y, target.reshape(-1))\n",
    "\n",
    "        # 역전파 진행\n",
    "        loss.backward()\n",
    "\n",
    "        # 가중치/절편 업데이트\n",
    "        OPTIMIZER.step()\n",
    "\n",
    "        E_LOSS += loss.item()\n",
    "        E_ACC  += acc.item()\n",
    "\n",
    "    return E_LOSS/ITERATION, E_ACC/ITERATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6]학습 진행 + 스케쥴링 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈로딩\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH[0/70]----------------\n",
      "- TRAIN_LOSS 0.71144  ACC 0.64815\n",
      "- VALID_LOSS 0.39091  ACC 0.96667\n",
      "[0] - num_bad_epochs : 0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH[1/70]----------------\n",
      "- TRAIN_LOSS 0.33598  ACC 0.92176\n",
      "- VALID_LOSS 0.24253  ACC 0.96667\n",
      "[1] - num_bad_epochs : 0 \n",
      "\n",
      "EPOCH[2/70]----------------\n",
      "- TRAIN_LOSS 0.20731  ACC 0.94954\n",
      "- VALID_LOSS 0.15624  ACC 0.93333\n",
      "[2] - num_bad_epochs : 0 \n",
      "\n",
      "EPOCH[3/70]----------------\n",
      "- TRAIN_LOSS 0.13959  ACC 0.97361\n",
      "- VALID_LOSS 0.16497  ACC 0.90000\n",
      "[3] - num_bad_epochs : 1 \n",
      "\n",
      "EPOCH[4/70]----------------\n",
      "- TRAIN_LOSS 0.13590  ACC 0.96204\n",
      "- VALID_LOSS 0.15028  ACC 0.90000\n",
      "[4] - num_bad_epochs : 0 \n",
      "\n",
      "EPOCH[5/70]----------------\n",
      "- TRAIN_LOSS 0.09834  ACC 0.96435\n",
      "- VALID_LOSS 0.21298  ACC 0.90000\n",
      "[5] - num_bad_epochs : 1 \n",
      "\n",
      "EPOCH[6/70]----------------\n",
      "- TRAIN_LOSS 0.15632  ACC 0.94444\n",
      "- VALID_LOSS 0.24644  ACC 0.90000\n",
      "[6] - num_bad_epochs : 2 \n",
      "\n",
      "EPOCH[7/70]----------------\n",
      "- TRAIN_LOSS 0.12664  ACC 0.96528\n",
      "- VALID_LOSS 0.41607  ACC 0.86667\n",
      "[7] - num_bad_epochs : 3 \n",
      "Early Stopping\n",
      "\n",
      "EPOCH[8/70]----------------\n",
      "- TRAIN_LOSS 0.23553  ACC 0.89815\n",
      "- VALID_LOSS 0.56677  ACC 0.83333\n",
      "[8] - num_bad_epochs : 0 \n",
      "\n",
      "EPOCH[9/70]----------------\n",
      "- TRAIN_LOSS 0.40260  ACC 0.87917\n",
      "- VALID_LOSS 0.14354  ACC 0.93333\n",
      "[9] - num_bad_epochs : 0 \n",
      "\n",
      "EPOCH[10/70]----------------\n",
      "- TRAIN_LOSS 0.09766  ACC 0.97500\n",
      "- VALID_LOSS 0.08068  ACC 0.96667\n",
      "[10] - num_bad_epochs : 0 \n",
      "\n",
      "EPOCH[11/70]----------------\n",
      "- TRAIN_LOSS 0.08463  ACC 0.96389\n",
      "- VALID_LOSS 0.08597  ACC 0.96667\n",
      "[11] - num_bad_epochs : 1 \n",
      "\n",
      "EPOCH[12/70]----------------\n",
      "- TRAIN_LOSS 0.07339  ACC 0.98889\n",
      "- VALID_LOSS 0.05609  ACC 1.00000\n",
      "[12] - num_bad_epochs : 0 \n",
      "\n",
      "EPOCH[13/70]----------------\n",
      "- TRAIN_LOSS 0.07146  ACC 0.97500\n",
      "- VALID_LOSS 0.05403  ACC 1.00000\n",
      "[13] - num_bad_epochs : 0 \n",
      "\n",
      "EPOCH[14/70]----------------\n",
      "- TRAIN_LOSS 0.07123  ACC 0.97500\n",
      "- VALID_LOSS 0.05559  ACC 1.00000\n",
      "[14] - num_bad_epochs : 1 \n",
      "\n",
      "EPOCH[15/70]----------------\n",
      "- TRAIN_LOSS 0.07016  ACC 0.97500\n",
      "- VALID_LOSS 0.05731  ACC 1.00000\n",
      "[15] - num_bad_epochs : 2 \n",
      "\n",
      "EPOCH[16/70]----------------\n",
      "- TRAIN_LOSS 0.06958  ACC 0.97500\n",
      "- VALID_LOSS 0.05640  ACC 1.00000\n",
      "[16] - num_bad_epochs : 3 \n",
      "Early Stopping\n",
      "\n",
      "EPOCH[17/70]----------------\n",
      "- TRAIN_LOSS 0.06924  ACC 0.97500\n",
      "- VALID_LOSS 0.05545  ACC 1.00000\n",
      "[17] - num_bad_epochs : 0 \n",
      "\n",
      "EPOCH[18/70]----------------\n",
      "- TRAIN_LOSS 0.06759  ACC 0.97500\n",
      "- VALID_LOSS 0.05537  ACC 1.00000\n",
      "[18] - num_bad_epochs : 1 \n",
      "\n",
      "EPOCH[19/70]----------------\n",
      "- TRAIN_LOSS 0.06756  ACC 0.97500\n",
      "- VALID_LOSS 0.05531  ACC 1.00000\n",
      "[19] - num_bad_epochs : 2 \n",
      "\n",
      "EPOCH[20/70]----------------\n",
      "- TRAIN_LOSS 0.06752  ACC 0.97500\n",
      "- VALID_LOSS 0.05526  ACC 1.00000\n",
      "[20] - num_bad_epochs : 3 \n",
      "Early Stopping\n",
      "\n",
      "EPOCH[21/70]----------------\n",
      "- TRAIN_LOSS 0.06748  ACC 0.97500\n",
      "- VALID_LOSS 0.05522  ACC 1.00000\n",
      "[21] - num_bad_epochs : 0 \n",
      "\n",
      "EPOCH[22/70]----------------\n",
      "- TRAIN_LOSS 0.06731  ACC 0.97500\n",
      "- VALID_LOSS 0.05521  ACC 1.00000\n",
      "[22] - num_bad_epochs : 1 \n",
      "\n",
      "EPOCH[23/70]----------------\n",
      "- TRAIN_LOSS 0.06731  ACC 0.97500\n",
      "- VALID_LOSS 0.05521  ACC 1.00000\n",
      "[23] - num_bad_epochs : 2 \n",
      "\n",
      "EPOCH[24/70]----------------\n",
      "- TRAIN_LOSS 0.06730  ACC 0.97500\n",
      "- VALID_LOSS 0.05520  ACC 1.00000\n",
      "[24] - num_bad_epochs : 3 \n",
      "Early Stopping\n",
      "\n",
      "EPOCH[25/70]----------------\n",
      "- TRAIN_LOSS 0.06730  ACC 0.97500\n",
      "- VALID_LOSS 0.05520  ACC 1.00000\n",
      "[25] - num_bad_epochs : 0 \n",
      "\n",
      "EPOCH[26/70]----------------\n",
      "- TRAIN_LOSS 0.06728  ACC 0.97500\n",
      "- VALID_LOSS 0.05519  ACC 1.00000\n",
      "[26] - num_bad_epochs : 1 \n",
      "\n",
      "EPOCH[27/70]----------------\n",
      "- TRAIN_LOSS 0.06728  ACC 0.97500\n",
      "- VALID_LOSS 0.05519  ACC 1.00000\n",
      "[27] - num_bad_epochs : 2 \n",
      "\n",
      "EPOCH[28/70]----------------\n",
      "- TRAIN_LOSS 0.06728  ACC 0.97500\n",
      "- VALID_LOSS 0.05519  ACC 1.00000\n",
      "[28] - num_bad_epochs : 3 \n",
      "Early Stopping\n",
      "28까지 학습후 성능 개선이 없어서 조기 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "# 에포크 : DS 처음부터 ~ 끝까지 학습  \n",
    "HIST ={'Train':[[],[]], 'Valid':[[],[]]}   \n",
    "\n",
    "\n",
    "# 학습 스케쥴러 생성\n",
    "lrScheduler = ReduceLROnPlateau(OPTIMIZER, patience=3, mode='min')\n",
    "# 조기종료  => 모델 성능 개선 없이 불필요한 학습 막기 위해서\n",
    "E_STOP_CNT = 5\n",
    "\n",
    "# 에포크 단위 학습/검증 진행 \n",
    "for epoch in range(EPOCHS):\n",
    "    trainLoss, trainAcc = training()\n",
    "    validLoss, validAcc = evaluate()\n",
    "\n",
    "    HIST['Train'][0].append(trainLoss)\n",
    "    HIST['Train'][1].append( trainAcc)\n",
    "\n",
    "    HIST['Valid'][0].append(validLoss)\n",
    "    HIST['Valid'][1].append(validAcc)\n",
    "\n",
    "    print(f'\\nEPOCH[{epoch}/{EPOCHS}]----------------')\n",
    "    print(f'- TRAIN_LOSS {trainLoss:.5f}  ACC {trainAcc:.5f}')\n",
    "    print(f'- VALID_LOSS {validLoss:.5f}  ACC {validAcc:.5f}')\n",
    "\n",
    "    lrScheduler.step(validLoss)\n",
    "\n",
    "    print(f'[{epoch}] - num_bad_epochs : {lrScheduler.num_bad_epochs} ')\n",
    "    if lrScheduler.num_bad_epochs == lrScheduler.patience:\n",
    "        print('Early Stopping')\n",
    "        E_STOP_CNT -=1\n",
    "\n",
    "    if not E_STOP_CNT:\n",
    "        print(f'{epoch}까지 학습후 성능 개선이 없어서 조기 종료합니다.')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_TORCH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
