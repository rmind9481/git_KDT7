{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ 동적 모델 설계 구현 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사용 모듈 : nn.ModuleList \n",
    "- 특징 : 일반 list로는 pytorch에서 layer 인식 안됨! ==> 대안 : ModuleList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------------------------------------------------------------------\n",
    "## 모델 설계\n",
    "## 입력층\n",
    "## 은닉층  <-- 유동적 0개 ~ N개 : 모델 인스턴스 생성 시 매개변수 전달 인자 \n",
    "## 출력층\n",
    "## -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모듈 로딩\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 동적 모델 클래스 정의 (1)\n",
    "class MyModel(nn.Module):\n",
    "\n",
    "    ## 동적 층 구성 초기화 메서드 \n",
    "    def __init__(self, in_in, out_out, h_in, h_cnt):\n",
    "        super().__init__()\n",
    "        # 입력층\n",
    "        self.input_layer= nn.Linear(in_in, h_in)\n",
    "        # 은닉층\n",
    "        self.h1_layer = nn.ModuleList( [ nn.Linear(h_in, h_in) for _ in range(h_cnt)])\n",
    "        # 출력층\n",
    "        self.output_layer = nn.Linear(h_in, out_out) \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 동적 모델 클래스 정의 (1) - 은닉층 개수 동적 \n",
    "class MyModel(nn.Module):\n",
    "\n",
    "    ## 동적 층 구성 초기화 메서드 \n",
    "    def __init__(self, in_in, out_out, h_in, h_cnt):\n",
    "        super().__init__()\n",
    "        # 입력층\n",
    "        self.input_layer= nn.Linear(in_in, h_in)\n",
    "        # 은닉층\n",
    "        self.h1_layer = nn.ModuleList( [ nn.Linear(h_in, h_in) for _ in range(h_cnt)])\n",
    "        # 출력층\n",
    "        self.output_layer = nn.Linear(h_in, out_out) \n",
    "\n",
    "    \n",
    "    ## 순전파 학습\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = F.relu(self.input_layer(x))\n",
    "\n",
    "        for hidden in self.h1_layer:\n",
    "            out = F.relu(hidden(out))\n",
    "\n",
    "        return self.output_layer(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MyModel                                  [100, 1]                  --\n",
       "├─Linear: 1-1                            [100, 5]                  20\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "│    └─Linear: 2-1                       [100, 5]                  30\n",
       "├─Linear: 1-3                            [100, 1]                  6\n",
       "==========================================================================================\n",
       "Total params: 56\n",
       "Trainable params: 56\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.01\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 모델 인스턴스 생성 및 테스트 \n",
    "# in_in  = 3   # out_out= 1  # hd_in  = 5   # hd_cnt = 1\n",
    "m1 = MyModel(3, 1, 5, 1)\n",
    "m1(torch.FloatTensor([[1,2,3]]))     # (샘플수, 피쳐수) \n",
    "\n",
    "summary(m1, input_size=(100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MyModel                                  [100, 3]                  --\n",
       "├─Linear: 1-1                            [100, 10]                 40\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "│    └─Linear: 2-1                       [100, 10]                 110\n",
       "│    └─Linear: 2-2                       [100, 10]                 110\n",
       "│    └─Linear: 2-3                       [100, 10]                 110\n",
       "│    └─Linear: 2-4                       [100, 10]                 110\n",
       "│    └─Linear: 2-5                       [100, 10]                 110\n",
       "├─Linear: 1-3                            [100, 3]                  33\n",
       "==========================================================================================\n",
       "Total params: 623\n",
       "Trainable params: 623\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.06\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.05\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.05\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in_in  = 3   # out_out= 3  # hd_in  = 10   # hd_cnt = 5\n",
    "m2 = MyModel(3, 3, 10, 5)\n",
    "m2(torch.FloatTensor([[1,2,3]]))     # (샘플수, 피쳐수) \n",
    "\n",
    "summary(m2, input_size=(100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 동적 모델 클래스 정의 (2) - 은닉층 개수 및 뉴런 개수 동적인 모델 -------------------\n",
    "## in_in   : 입력층의 입력   in_out    : 입력층의 출력 / 다음층의 입력\n",
    "## out_out : 출력층의 출력   h_outs=[] : 은닉층의 출력 리스트\n",
    "class DynamicModel(nn.Module):\n",
    "    def __init__(self, in_in, in_out, out_out, h_outs=[]):\n",
    "        # 부모클래스 생성\n",
    "        super().__init__()\n",
    "        # 입력층 생성\n",
    "        self.input_layer= nn.Linear( in_in,  in_out ) \n",
    "\n",
    "        # 은닉층 여러개 생성\n",
    "        self.h1_layer=nn.ModuleList()\n",
    "        for idx in range(len(h_outs)):  \n",
    "            h_in  = h_outs[idx-1] if idx else  in_out\n",
    "            h_out = h_outs[idx]\n",
    "            self.h1_layer.append( nn.Linear(h_in, h_out) )\n",
    "\n",
    "        # 출력층 생성\n",
    "        self.output_layer=nn.Linear( h_outs[-1] if len(h_outs) else in_out,  out_out )\n",
    "\n",
    "    # 순전파 학습 메서드 -------------------------------------------------\n",
    "    def forward(self, x):\n",
    "        y=F.relu(self.input_layer(x))\n",
    "        \n",
    "        for linear in self.h1_layer: \n",
    "            y=F.relu(linear(y))\n",
    "            \n",
    "        return self.output_layer(y)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DynamicModel                             [100, 1]                  --\n",
       "├─Linear: 1-1                            [100, 5]                  20\n",
       "├─Linear: 1-2                            [100, 1]                  6\n",
       "==========================================================================================\n",
       "Total params: 26\n",
       "Trainable params: 26\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 모델 인스턴스 생성 및 테스트 \n",
    "## in_in   : 입력층의 입력   in_out    : 입력층의 출력 / 다음층의 입력\n",
    "## out_out : 출력층의 출력   h_outs=[] : 은닉층의 출력 리스트\n",
    "m1 = DynamicModel(3, 5, 1)\n",
    "m1(torch.FloatTensor([[1,2,3]]))     # (샘플수, 피쳐수) \n",
    "\n",
    "summary(m1, input_size=(100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DynamicModel                             [100, 3]                  --\n",
       "├─Linear: 1-1                            [100, 10]                 40\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "│    └─Linear: 2-1                       [100, 7]                  77\n",
       "│    └─Linear: 2-2                       [100, 5]                  40\n",
       "├─Linear: 1-3                            [100, 3]                  18\n",
       "==========================================================================================\n",
       "Total params: 175\n",
       "Trainable params: 175\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.02\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.02\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.02\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 모델 인스턴스 생성 및 테스트 \n",
    "## in_in   : 입력층의 입력   in_out    : 입력층의 출력 / 다음층의 입력\n",
    "## out_out : 출력층의 출력   h_outs=[] : 은닉층의 출력 리스트\n",
    "m1 = DynamicModel(3, 10, 3, [7, 5])\n",
    "m1(torch.FloatTensor([[1,2,3]]))     # (샘플수, 피쳐수) \n",
    "\n",
    "summary(m1, input_size=(100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[input_layer.weight]-------------------\n",
      "Parameter containing:\n",
      "tensor([[-0.3934, -0.0091, -0.2757],\n",
      "        [ 0.0559, -0.4245,  0.5056],\n",
      "        [ 0.5641, -0.1437,  0.4157],\n",
      "        [-0.5158,  0.2054, -0.0737],\n",
      "        [ 0.2024, -0.2727, -0.1350],\n",
      "        [-0.1544,  0.1046,  0.3444],\n",
      "        [-0.2588, -0.5181,  0.3021],\n",
      "        [-0.5072, -0.3291,  0.3278],\n",
      "        [-0.3733,  0.0798,  0.4676],\n",
      "        [ 0.3100, -0.0990,  0.0575]], requires_grad=True)\n",
      "[input_layer.bias]-------------------\n",
      "Parameter containing:\n",
      "tensor([ 0.3346,  0.3254,  0.1174,  0.3424,  0.3860, -0.0801,  0.4711, -0.0054,\n",
      "        -0.2339, -0.1561], requires_grad=True)\n",
      "[h1_layer.0.weight]-------------------\n",
      "Parameter containing:\n",
      "tensor([[ 0.0963,  0.1053,  0.0207,  0.2670, -0.0724, -0.2170,  0.1538, -0.0899,\n",
      "         -0.2465,  0.1174],\n",
      "        [ 0.2722,  0.1705, -0.1623,  0.1700, -0.0993, -0.2354, -0.1813, -0.2854,\n",
      "         -0.0235, -0.2137],\n",
      "        [ 0.2249,  0.2810, -0.2455, -0.1945,  0.2742, -0.3113,  0.3123,  0.0413,\n",
      "          0.0112,  0.1921],\n",
      "        [ 0.0794, -0.0968,  0.0932,  0.2511,  0.0953,  0.1263,  0.2428, -0.0059,\n",
      "          0.0730, -0.1586],\n",
      "        [ 0.1703,  0.2188,  0.1319, -0.2810,  0.0123,  0.2758,  0.1362, -0.0918,\n",
      "         -0.1339,  0.0911],\n",
      "        [ 0.0654, -0.1087, -0.0486,  0.2947,  0.3140, -0.3098, -0.1445,  0.2384,\n",
      "          0.1836, -0.2409],\n",
      "        [-0.1084,  0.0886,  0.2533, -0.2190, -0.0717,  0.2045, -0.0465, -0.0336,\n",
      "          0.2309,  0.0630]], requires_grad=True)\n",
      "[h1_layer.0.bias]-------------------\n",
      "Parameter containing:\n",
      "tensor([-0.1165,  0.2862, -0.0929,  0.0398, -0.0831,  0.3134, -0.0137],\n",
      "       requires_grad=True)\n",
      "[h1_layer.1.weight]-------------------\n",
      "Parameter containing:\n",
      "tensor([[-0.1212,  0.3534, -0.1287,  0.0777,  0.2669, -0.1420, -0.2117],\n",
      "        [ 0.3646, -0.1470, -0.2656, -0.0028,  0.3734, -0.2747, -0.2066],\n",
      "        [ 0.3698,  0.3277, -0.1624, -0.3512,  0.3176, -0.2217, -0.0897],\n",
      "        [-0.0959, -0.2756, -0.0252,  0.1698,  0.2204, -0.2313, -0.1417],\n",
      "        [ 0.0379, -0.0519, -0.0996,  0.1978, -0.3572, -0.2948, -0.3264]],\n",
      "       requires_grad=True)\n",
      "[h1_layer.1.bias]-------------------\n",
      "Parameter containing:\n",
      "tensor([-0.2772,  0.2229,  0.3410, -0.2715,  0.0907], requires_grad=True)\n",
      "[output_layer.weight]-------------------\n",
      "Parameter containing:\n",
      "tensor([[-0.0406,  0.3391,  0.3188, -0.2324,  0.4222],\n",
      "        [-0.3231,  0.2008, -0.4072, -0.3805, -0.2114],\n",
      "        [ 0.1862, -0.3481, -0.1897, -0.2031,  0.3629]], requires_grad=True)\n",
      "[output_layer.bias]-------------------\n",
      "Parameter containing:\n",
      "tensor([-0.2036,  0.2648, -0.3305], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param in m1.named_parameters():\n",
    "    print(f'[{name}]-------------------\\n{param}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.choices = nn.ModuleDict({\n",
    "                    'conv': nn.Conv2d(10, 10, 3),\n",
    "                    'pool': nn.MaxPool2d(3)\n",
    "            })\n",
    "            self.activations = nn.ModuleDict([\n",
    "                    ['lrelu', nn.LeakyReLU()],\n",
    "                    ['prelu', nn.PReLU()]\n",
    "            ])\n",
    "\n",
    "        def forward(self, x, choice, act):\n",
    "            x = self.choices[choice](x)\n",
    "            x = self.activations[act](x)\n",
    "            return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_TORCH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
